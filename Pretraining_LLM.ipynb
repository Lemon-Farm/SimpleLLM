{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fnf00\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f71f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    text_path = 'dataset/sherlock-holm.es_stories_plain-text_advs.txt'\n",
    "    max_length = 128\n",
    "    stride = 4\n",
    "    batch_size = 16\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a351fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed text length: 558453, words: 104321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (133741 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133741\n"
     ]
    }
   ],
   "source": [
    "# 전처리 및 토크나이즈\n",
    "with open(cfg.text_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 줄 앞뒤 공백 제거 및 연속된 빈 줄 1개로 정리\n",
    "raw_text = re.sub(r'\\s+\\n', '\\n', raw_text)\n",
    "raw_text = re.sub(r'^\\s+', '', raw_text, flags=re.MULTILINE)\n",
    "raw_text = re.sub(r'\\n{2,}', '\\n\\n', raw_text)\n",
    "\n",
    "# 제목 삭제\n",
    "raw_text = re.sub(r'THE ADVEN.*\\n', '', raw_text, flags=re.MULTILINE)\n",
    "\n",
    "# 목차나 저자 등 메타데이터 제거\n",
    "raw_text = re.sub(r'Arthur Conan Doyle', '', raw_text, flags=re.IGNORECASE)\n",
    "raw_text = re.sub(r'\\bTable of contents\\b.*?(?=CHAPTER I)', '', raw_text, flags=re.DOTALL|re.IGNORECASE)\n",
    "raw_text = re.sub(r'chapter ([0~9]|[ivx])', '', raw_text, flags=re.IGNORECASE)\n",
    "raw_text = re.sub(r'----.*', '', raw_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "# 특수문자, 과도한 빈칸 정리\n",
    "raw_text = re.sub(r'[“”]', '\"', raw_text)\n",
    "raw_text = re.sub(r\"[‘’]\", \"'\", raw_text)\n",
    "raw_text = re.sub(r' +', ' ', raw_text)\n",
    "\n",
    "# 앞뒤 전체 공백 제거\n",
    "raw_text = raw_text.strip()\n",
    "\n",
    "# 전처리된 텍스트 결과\n",
    "preprocessed_text = raw_text\n",
    "print(f\"preprocessed text length: {len(preprocessed_text)}, words: {len(preprocessed_text.split())}\")\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 토크나이즈\n",
    "tokenized_text = tokenizer.encode(preprocessed_text)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c6b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset) :\n",
    "    def __init__(self, tokenized_text, cfg) :\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(0, len(self.tokenized_text) - self.cfg.max_length, self.cfg.stride) :\n",
    "            self.input_ids.append(self.tokenized_text[i:i+self.cfg.max_length])\n",
    "            self.labels.append(self.tokenized_text[i+1:i+self.cfg.max_length+1])\n",
    "        \n",
    "        self.input_ids = torch.tensor(self.input_ids)\n",
    "        self.labels = torch.tensor(self.labels)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.input_ids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2498f24",
   "metadata": {},
   "source": [
    "모델 참조 : https://www.manning.com/books/build-a-large-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6ee931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "#VOCAB_SIZE = len(tokenizer) # AutoTokenizer\n",
    "L = 128  # Shortened context length (orig: 1024)\n",
    "E = 768  # Embedding dimension\n",
    "H = 12  # Number of attention heads\n",
    "NUM_LAYERS = 12  # Number of layers\n",
    "DROP_RATE = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078847d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module) :\n",
    "    def __init__(self, din, dout) :\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dout % H == 0, \"dout must be devided by NUM_HEADS\"\n",
    "        \n",
    "        self.din = din\n",
    "        self.dout = dout\n",
    "        self.D = dout // H\n",
    "        \n",
    "        self.W_Q = nn.Linear(din, dout)\n",
    "        self.W_K = nn.Linear(din, dout)\n",
    "        self.W_V = nn.Linear(din, dout)\n",
    "        self.out_proj = nn.Linear(dout, dout)\n",
    "        self.dropout = nn.Dropout(p=DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(L, L), diagonal=1)) # Causal Attn을 위한 Mask\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        B, L, E = x.shape\n",
    "        D = self.D\n",
    "        \n",
    "        # Query, Key, Value Matrix 생성 (B, L, E)\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        # Multihead 반영 (B, L, E) -> (B, L, H, D) -> (B, H, L, D)\n",
    "        Q = Q.view(B, L, H, D).transpose(1, 2)\n",
    "        K = K.view(B, L, H, D).transpose(1, 2)\n",
    "        V = V.view(B, L, H ,D).transpose(1, 2)\n",
    "        \n",
    "        # Attention Score 구하기\n",
    "        attn_scores = Q @ K.transpose(2, 3) # (B, H, L, L)\n",
    "        attn_scores.masked_fill(self.mask[:L, :L] == 1, float('-inf'))\n",
    "        attn_scores = attn_scores / (D ** 0.5)\n",
    "        \n",
    "        # Attention Weight 구하기\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Context Vector 구하기\n",
    "        CV = attn_weights @ V # (B, H, L, D)\n",
    "        CV = CV.reshape(B, L, E)\n",
    "        CV = self.out_proj(CV)\n",
    "        \n",
    "        return CV\n",
    "    \n",
    "class LayerNorm(nn.Module) :\n",
    "    def __init__(self, dim, eps=1e-5) :\n",
    "        super().__init__()\n",
    "        \n",
    "        # 파라미터 생성\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps) * self.gamma + self.beta\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(E, 4*E),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*E, E),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(E, E)\n",
    "        self.norm = LayerNorm(E)\n",
    "        self.ff = FeedForward()\n",
    "        self.dropout = nn.Dropout(DROP_RATE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SimpleLLM(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(VOCAB_SIZE, E)\n",
    "        self.positional_embedding = nn.Embedding(L, E)\n",
    "        self.dropout_embedding = nn.Dropout(DROP_RATE)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock() for _ in range(NUM_LAYERS)]\n",
    "        )\n",
    "        \n",
    "        self.last_norm = LayerNorm(E)\n",
    "        self.out = nn.Linear(E, VOCAB_SIZE, bias=False)\n",
    "        \n",
    "    def forward(self, input_ids) :\n",
    "        B, L = input_ids.shape\n",
    "        tok_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.positional_embedding(torch.arange(L, device=input_ids.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.dropout_embedding(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.last_norm(x)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cea0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch : 1]:  40%|███▉      | 831/2088 [02:46<04:11,  5.00it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py\", line 3549, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Local\\Temp\\ipykernel_20248\\2669281967.py\", line 31, in <module>\n",
      "    pbar.set_postfix(loss=loss.item())\n",
      "                          ~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py\", line 2173, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "        etype, value, tb, tb_offset=tb_offset\n",
      "    )\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, evalue, etb, tb_offset, context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 1053, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, etype, evalue, etb, tb_offset, context\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 861, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        etype, evalue, etb, context, tb_offset\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 773, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\ultratb.py\", line 652, in format_record\n",
      "    frame_info.lines,\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\tbtools.py\", line 355, in lines\n",
      "    return self._sd.lines  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ~~~~~~~~~^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fnf00\\AppData\\Roaming\\Python\\Python313\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = SimpleLLM().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4, weight_decay=0.1)\n",
    "train_dataset = MyDataset(tokenized_text, cfg)\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"[Epoch : {epoch + 1}]\")\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for input_ids, labels in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, labels = input_ids.to(device), labels.to(device) # (B, L)\n",
    "        \n",
    "        logits = model(input_ids) # (B, L, V)\n",
    "        loss = F.cross_entropy(logits.flatten(0, 1), labels.flatten())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = epoch_loss / len(pbar)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f\"Avg Loss : {avg_loss}\")\n",
    "    torch.save(model.state_dict(), f\"model/simplellm_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 출력\n",
    "plt.plot(epoch_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2255628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성\n",
    "\n",
    "# 모델 불러오기\n",
    "model.load_state_dict(torch.load('asdf.pth', map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "text = \"It was evident\" # 시작 문장\n",
    "\n",
    "input_ids = tokenizer.encode(text)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0) # (1, L)\n",
    "\n",
    "generation_len = 10\n",
    "for _ in range(generation_len) :\n",
    "    with torch.no_grads() :\n",
    "        logits = model(input_ids) # (1, L, V)\n",
    "    logits = logits[:, -1, :] # 마지막 토큰(새로 생성된 토큰만 추출), (1, V) -> 정수 인덱싱 시 차원 삭제됨됨\n",
    "    \n",
    "    next_token = torch.argmax(logits, dim=-1, keepdim=True) # (1, 1)\n",
    "    input_ids = torch.cat((input_ids, next_token), dim=-1)\n",
    "    \n",
    "input_ids = input_ids.squeeze(0) # (Generation_Len, )\n",
    "generated_text = tokenizer.decode(input_ids.tolist().replace(\"\\n\", \" \"))\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
