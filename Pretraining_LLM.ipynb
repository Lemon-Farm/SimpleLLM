{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fnf00\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f71f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    text_path = 'dataset/sherlock-holm.es_stories_plain-text_advs.txt'\n",
    "    max_length = 128\n",
    "    stride = 4\n",
    "    batch_size = 128\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a351fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed text length: 558453, words: 104321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (133741 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133741\n"
     ]
    }
   ],
   "source": [
    "# 전처리 및 토크나이즈\n",
    "with open(cfg.text_path, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# 줄 앞뒤 공백 제거 및 연속된 빈 줄 1개로 정리\n",
    "raw_text = re.sub(r'\\s+\\n', '\\n', raw_text)\n",
    "raw_text = re.sub(r'^\\s+', '', raw_text, flags=re.MULTILINE)\n",
    "raw_text = re.sub(r'\\n{2,}', '\\n\\n', raw_text)\n",
    "\n",
    "# 제목 삭제\n",
    "raw_text = re.sub(r'THE ADVEN.*\\n', '', raw_text, flags=re.MULTILINE)\n",
    "\n",
    "# 목차나 저자 등 메타데이터 제거\n",
    "raw_text = re.sub(r'Arthur Conan Doyle', '', raw_text, flags=re.IGNORECASE)\n",
    "raw_text = re.sub(r'\\bTable of contents\\b.*?(?=CHAPTER I)', '', raw_text, flags=re.DOTALL|re.IGNORECASE)\n",
    "raw_text = re.sub(r'chapter ([0~9]|[ivx])', '', raw_text, flags=re.IGNORECASE)\n",
    "raw_text = re.sub(r'----.*', '', raw_text, flags=re.DOTALL)\n",
    "\n",
    "\n",
    "# 특수문자, 과도한 빈칸 정리\n",
    "raw_text = re.sub(r'[“”]', '\"', raw_text)\n",
    "raw_text = re.sub(r\"[‘’]\", \"'\", raw_text)\n",
    "raw_text = re.sub(r' +', ' ', raw_text)\n",
    "\n",
    "# 앞뒤 전체 공백 제거\n",
    "raw_text = raw_text.strip()\n",
    "\n",
    "# 전처리된 텍스트 결과\n",
    "preprocessed_text = raw_text\n",
    "print(f\"preprocessed text length: {len(preprocessed_text)}, words: {len(preprocessed_text.split())}\")\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 토크나이즈\n",
    "tokenized_text = tokenizer.encode(preprocessed_text)\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c6b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset) :\n",
    "    def __init__(self, tokenized_text, cfg) :\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.cfg = cfg\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.tokenized_text)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(0, len(self.tokenized_text) - self.cfg.max_length, self.cfg.stride) :\n",
    "            input_ids.append(self.tokenized_text[i:i+self.cfg.max_length])\n",
    "            labels.append(self.tokenized_text[i+1:i+self.cfg.max_length+1])\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        return input_ids, labels\n",
    "    \n",
    "train_dataset = MyDataset(tokenized_text, cfg)\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2498f24",
   "metadata": {},
   "source": [
    "모델 참조 : https://www.manning.com/books/build-a-large-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b6ee931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "#VOCAB_SIZE = len(tokenizer) # AutoTokenizer\n",
    "CONTEXT_LENGTH = 128  # Shortened context length (orig: 1024)\n",
    "E = 768  # Embedding dimension\n",
    "H = 12  # Number of attention heads\n",
    "NUM_LAYERS = 12  # Number of layers\n",
    "DROP_RATE = 0.1  # Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078847d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module) :\n",
    "    def __init__(self, din, dout) :\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dout % H == 0, \"dout must be devided by NUM_HEADS\"\n",
    "        \n",
    "        self.din = din\n",
    "        self.dout = dout\n",
    "        self.D = dout // H\n",
    "        \n",
    "        self.W_Q = nn.Linear(din, dout)\n",
    "        self.W_K = nn.Linear(din, dout)\n",
    "        self.W_V = nn.Linear(din, dout)\n",
    "        self.out_proj = nn.Linear(dout, dout)\n",
    "        self.dropout = nn.Dropout(p=DROP_RATE)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH), diagonal=1)) # Causal Attn을 위한 Mask\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        B, L, E = x.shape\n",
    "        D = self.D\n",
    "        \n",
    "        # Query, Key, Value Matrix 생성 (B, L, E)\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        # Multihead 반영 (B, L, E) -> (B, L, H, D) -> (B, H, L, D)\n",
    "        Q = Q.view(B, L, H, D).transpose(1, 2)\n",
    "        K = K.view(B, L, H, D).transpose(1, 2)\n",
    "        V = V.view(B, L, H ,D).transpose(1, 2)\n",
    "        \n",
    "        # Attention Score 구하기\n",
    "        attn_scores = Q @ K.transpose(2, 3) # (B, H, L, L)\n",
    "        attn_scores.masked_fill(self.mask[:L, :L] == 1, float('-inf'))\n",
    "        attn_scores = attn_scores / (D ** 0.5)\n",
    "        \n",
    "        # Attention Weight 구하기\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Context Vector 구하기\n",
    "        CV = attn_weights @ V # (B, H, L, D)\n",
    "        CV = CV.reshape(B, L, E)\n",
    "        CV = self.out_proj(CV)\n",
    "        \n",
    "        return CV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
